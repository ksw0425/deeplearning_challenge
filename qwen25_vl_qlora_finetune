# qwen25_vl_qlora_finetune/trainer.py
# transformers==4.55.2, bitsandbytes==0.47.0
# Qwen2.5-VL-7B ‚Äî Unified Multitask Fine-tuning (QLoRA)
import os, io, re, csv, json, base64, hashlib, warnings, argparse, inspect, random
from dataclasses import dataclass
from typing import List, Dict, Any, Optional, Tuple

import numpy as np
import pandas as pd
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset
from PIL import Image, ImageFile

from transformers import (
    AutoModelForVision2Seq, AutoModelForCausalLM,
    AutoProcessor, AutoTokenizer,
    BitsAndBytesConfig, Trainer, TrainingArguments,
)

from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# ========= Safety / Env =========
os.environ.setdefault("PYTORCH_CUDA_ALLOC_CONF", "max_split_size_mb=128")
ImageFile.LOAD_TRUNCATED_IMAGES = True
warnings.filterwarnings("ignore", category=UserWarning, module="PIL")

# ========= System prompt =========
SYSTEM_PROMPT = (
    "You are a single vision‚Äìlanguage assistant. You will receive either an image or text, and optionally a question.\n"
    "Always reply in English in a single, unified style across tasks.\n"
    "If the answer cannot be determined from the provided input, output exactly: unknown."
)

# ========= Image I/O =========
from urllib.parse import urlparse
from urllib.request import urlopen, Request

URL_TIMEOUT = 20
LONG_SIDE = 1280
MAX_SIDE_HARD = 3500
MIN_IMG_SIDE = 64
RESIZE_CACHE_DIR = "/tmp/img_resized_cache"
os.makedirs(RESIZE_CACHE_DIR, exist_ok=True)

_b64_re = re.compile(r'^[A-Za-z0-9+/=\n\r]+$')

def looks_like_base64(s: str, min_len: int = 128) -> bool:
    if not (isinstance(s, str) and len(s) >= min_len and _b64_re.match(s)):
        return False
    try:
        head = base64.b64decode(s[:4096], validate=True)
        return head.startswith(b"\x89PNG") or head.startswith(b"\xff\xd8")
    except Exception:
        return False

def is_url(path: str) -> bool:
    try:
        return urlparse(str(path)).scheme in ("http", "https")
    except Exception:
        return False

def _cap_max_side(img: Image.Image, cap=MAX_SIDE_HARD) -> Image.Image:
    if max(img.size) <= cap:
        return img
    img = img.copy()
    img.thumbnail((cap, cap), Image.LANCZOS)
    return img

def _resize_keep_ratio(img: Image.Image, long_side: int) -> Image.Image:
    if max(img.size) <= long_side:
        return img
    img = img.copy()
    img.thumbnail((long_side, long_side), Image.LANCZOS)
    return img

def _hash_image_bytes(img: Image.Image) -> str:
    with io.BytesIO() as bio:
        img.save(bio, format="PNG", optimize=False)
        return hashlib.md5(bio.getvalue()).hexdigest()

def finalize_image(img: Image.Image) -> Image.Image:
    img = _cap_max_side(img, MAX_SIDE_HARD)
    target = LONG_SIDE
    if max(img.size) <= target:
        return img
    h = _hash_image_bytes(img) + f"_{target}"
    path = os.path.join(RESIZE_CACHE_DIR, h + ".png")
    if os.path.exists(path):
        return Image.open(path).convert("RGB")
    out = _resize_keep_ratio(img, target)
    out.save(path, format="PNG")
    return out

def load_image_strict(input_obj) -> Image.Image:
    """Load an image, raise on failure. No 1√ó1 fallbacks."""
    try:
        if isinstance(input_obj, (bytes, bytearray)):
            img = Image.open(io.BytesIO(input_obj)).convert("RGB")
        elif isinstance(input_obj, str):
            s = input_obj.strip()
            if s.startswith("data:image"):
                b64 = s.split(",", 1)[1]
                img = Image.open(io.BytesIO(base64.b64decode(b64))).convert("RGB")
            elif looks_like_base64(s):
                img = Image.open(io.BytesIO(base64.b64decode(s))).convert("RGB")
            elif is_url(s):
                req = Request(s, headers={"User-Agent": "Mozilla/5.0"})
                with urlopen(req, timeout=URL_TIMEOUT) as r:
                    raw = r.read()
                img = Image.open(io.BytesIO(raw)).convert("RGB")
            else:
                img = Image.open(s).convert("RGB")
        else:
            raise TypeError(f"Unsupported image input type: {type(input_obj)}")
    except Exception as e:
        raise RuntimeError(f"[ImageLoadError] {input_obj!r} -> {e}")

    img = finalize_image(img)
    w, h = img.size
    if min(w, h) < MIN_IMG_SIDE:
        raise ValueError(f"[ImageTooSmall] {w}x{h} < {MIN_IMG_SIDE}")
    return img

# ========= Prompt helpers =========
def build_user_prompt(task: str, input_type: str, the_input: str, question: Optional[str]) -> str:
    t = (task or "").strip().lower()
    it = (input_type or "text").strip().lower()
    q = (question or "").strip()
    lines = [
        f"Task: {t}",
        f"InputType: {it}",
        f"Question: {q}" if q else "Question:",
    ]
    lines.append("Input:\n" + (the_input or "") if it == "text" else "Input: <image>")
    return "\n".join(lines)

def build_messages(inp_type: str, task: str, inp: str, question: str, output: Optional[str] = None):
    user_prompt = build_user_prompt(task, inp_type, str(inp), question)
    if inp_type == "image":
        img = load_image_strict(inp)
        user_content = [{"type": "image"}, {"type": "text", "text": user_prompt}]
        images = [img]
    else:
        user_content = [{"type": "text", "text": user_prompt}]
        images = None

    msgs = [
        {"role": "system", "content": [{"type": "text", "text": SYSTEM_PROMPT}]},
        {"role": "user",   "content": user_content},
    ]
    if output is not None:
        msgs.append({"role": "assistant", "content": [{"type": "text", "text": str(output).rstrip()}]})
    return msgs, images

# ========= Dataset & Collator =========
class MMDataset(Dataset):
    def __init__(self, df: pd.DataFrame, processor: AutoProcessor):
        self.df = df.reset_index(drop=True)
        self.p = processor
        self.max_len = getattr(self.p.tokenizer, "model_max_length", 4096)

    def __len__(self): return len(self.df)

    def __getitem__(self, idx: int) -> Dict[str, Any]:
        row = self.df.iloc[idx]
        inp_type = str(row["input_type"]).strip().lower()
        task     = str(row["task"])
        inp      = row["input"]
        q        = row.get("question", "") or ""
        out      = str(row.get("output", "")).rstrip()

        msgs_prompt, images = build_messages(inp_type, task, inp, q, output=None)
        msgs_full,   _      = build_messages(inp_type, task, inp, q, output=out)

        prompt_text = self.p.apply_chat_template(msgs_prompt, tokenize=False, add_generation_prompt=False)
        full_text   = self.p.apply_chat_template(msgs_full,   tokenize=False, add_generation_prompt=False)

        if images is not None:
            enc_full = self.p(text=full_text, images=images, return_tensors="pt", padding="longest")
            enc_prm  = self.p(text=prompt_text, images=images, return_tensors="pt", padding="longest")
        else:
            enc_full = self.p(text=full_text, return_tensors="pt", padding="longest",
                              truncation=True, max_length=self.max_len)
            enc_prm  = self.p(text=prompt_text, return_tensors="pt", padding="longest",
                              truncation=True, max_length=self.max_len)

        input_ids = enc_full["input_ids"][0]
        labels = input_ids.clone()
        prm_len = enc_prm["input_ids"].shape[-1]
        labels[:prm_len] = -100
        
        item = {
            "input_ids": input_ids,
            "attention_mask": enc_full["attention_mask"][0],
            "labels": labels,
        }
        
        # üîß pixel_values Ï†ïÍ∑úÌôî: (C,H,W) ÎòêÎäî (N,C,H,W)Îßå ÌóàÏö©
        pv = enc_full.get("pixel_values", None)
        
        if isinstance(pv, list):
            pv = pv[0] if len(pv) > 0 else None
        
        if isinstance(pv, torch.Tensor):
            # (1,C,H,W) ‚Üí (C,H,W)
            if pv.dim() == 4 and pv.size(0) == 1:
                pv = pv.squeeze(0)
            # Ï±ÑÎÑê 1Í∞úÎ©¥ 3Ï±ÑÎÑêÎ°ú ÌôïÏû•
            if pv.dim() == 3 and pv.size(0) == 1:
                pv = pv.repeat(3, 1, 1)
        
            if pv.dim() == 3 or pv.dim() == 4:
                item["pixel_values"] = pv
        return item

@dataclass
class QwenVLDataCollator:
    pad_token_id: int = 0

    def __call__(self, features):
        # ---- ÌÖçÏä§Ìä∏ Ìå®Îî© ----
        input_ids      = [f["input_ids"]      for f in features]
        attention_mask = [f["attention_mask"] for f in features]
        labels         = [f["labels"]         for f in features]
        max_len = max(x.size(0) for x in input_ids)

        def pad1d(x, val):
            if x.size(0) < max_len:
                pad = torch.full((max_len-x.size(0),), val, dtype=x.dtype, device=x.device)
                return torch.cat([x, pad], dim=0)
            return x

        batch = {
            "input_ids":      torch.stack([pad1d(x, self.pad_token_id) for x in input_ids]),
            "attention_mask": torch.stack([pad1d(x, 0)                 for x in attention_mask]),
            "labels":         torch.stack([pad1d(x, -100)              for x in labels]),
        }

        # ---- ÌîΩÏÖÄ Ìå®Îî© (3D/None) ----
        pvs = [f.get("pixel_values", None) for f in features]
        if any(isinstance(pv, torch.Tensor) for pv in pvs):
            # Ïú†Ìö®Ìïú 3DÎßå Ï∑®Í∏â, ÎÇòÎ®∏ÏßÄÎäî None
            norm = []
            Hs, Ws = [], []
            for pv in pvs:
                if isinstance(pv, torch.Tensor) and pv.dim() == 3:
                    c, h, w = pv.shape
                    # Ï±ÑÎÑê Î≥¥Ï†ï(ÌòπÏãú c==1Î°ú Ïò¨ ÏàòÎèÑ ÏûàÏùå)
                    if c == 1:
                        pv = pv.repeat(3, 1, 1)
                        c, h, w = pv.shape
                    norm.append(pv); Hs.append(h); Ws.append(w)
                else:
                    norm.append(None)
            if len(Hs) > 0:
                max_h, max_w = max(Hs), max(Ws)
                padded = []
                for pv in norm:
                    if pv is None:
                        pv = torch.zeros((3, max_h, max_w), dtype=batch["input_ids"].dtype)
                    else:
                        c, h, w = pv.shape
                        if h != max_h or w != max_w:
                            pv = F.pad(pv, (0, max_w-w, 0, max_h-h), value=0)
                    padded.append(pv)
                batch["pixel_values"] = torch.stack(padded)  # (B, C, H, W)

        return batch

# ========= QLoRA config =========
def make_bnb_config() -> BitsAndBytesConfig:
    return BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype=torch.bfloat16,
    )

def make_lora_config(r=64, alpha=128, dropout=0.05, target_modules: Optional[List[str]] = None) -> LoraConfig:
    if target_modules is None:
        target_modules = ["q_proj","k_proj","v_proj","o_proj","up_proj","down_proj","gate_proj"]
    return LoraConfig(r=r, lora_alpha=alpha, lora_dropout=dropout, target_modules=target_modules,
                      bias="none", task_type="CAUSAL_LM")

# ========= Utilities =========
def set_seed(seed: int = 42):
    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)

def read_any(path: str) -> pd.DataFrame:
    if path.endswith(".parquet"): return pd.read_parquet(path)
    if path.endswith(".jsonl"):
        with open(path, "r", encoding="utf-8") as f:
            return pd.DataFrame(json.loads(l) for l in f if l.strip())
    return pd.read_csv(path)

# ========= Train (4.55.2-safe) =========
def train(
    base_model: str,
    train_path: str,
    valid_path: Optional[str],
    out_dir: str,
    profile: str = "base",          # "dev" | "base" | "long"
    lora_r: int = 64,
    lora_alpha: int = 128,
    lora_dropout: float = 0.05,
):
    set_seed(42)
    os.makedirs(out_dir, exist_ok=True)

    # Processor/tokenizer
    processor = AutoProcessor.from_pretrained(base_model, trust_remote_code=True)
    tokenizer = getattr(processor, "tokenizer", None) or AutoTokenizer.from_pretrained(
        base_model, use_fast=True, trust_remote_code=True
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # Base model (Vision2Seq -> CausalLM fallback)
    bnb_cfg = make_bnb_config()
    try:
        model = AutoModelForVision2Seq.from_pretrained(
            base_model, device_map="auto", torch_dtype=torch.bfloat16,
            quantization_config=bnb_cfg, trust_remote_code=True,
        )
    except Exception as e1:
        try:
            model = AutoModelForCausalLM.from_pretrained(
                base_model, device_map="auto", torch_dtype=torch.bfloat16,
                quantization_config=bnb_cfg, trust_remote_code=True,
            )
        except Exception as e2:
            raise RuntimeError(f"Failed to load base model: {e1} / {e2}")

    model.config.use_cache = False
    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)
    lora_cfg = make_lora_config(r=lora_r, alpha=lora_alpha, dropout=lora_dropout)
    model = get_peft_model(model, lora_cfg)

    # Data
    train_df = read_any(train_path)
    valid_df = read_any(valid_path) if valid_path else None
    need = {"task","input_type","input","question","output"}
    if not need.issubset(set(train_df.columns)):
        raise ValueError(f"Train file missing columns: {need - set(train_df.columns)}")
    if valid_df is not None and not need.issubset(set(valid_df.columns)):
        raise ValueError(f"Valid file missing columns: {need - set(valid_df.columns)}")

    train_ds = MMDataset(train_df, processor)
    eval_ds  = MMDataset(valid_df, processor) if valid_df is not None else None
    collator = QwenVLDataCollator(pad_token_id=tokenizer.pad_token_id)

    profiles = {
        "dev":  dict(num_train_epochs=1, per_device_train_batch_size=1, per_device_eval_batch_size=1,
                     gradient_accumulation_steps=8,  learning_rate=1e-4, warmup_ratio=0.03,
                     logging_steps=50, eval_steps=500, save_steps=1000, weight_decay=0.0),
        "base": dict(num_train_epochs=2, per_device_train_batch_size=1, per_device_eval_batch_size=1,
                     gradient_accumulation_steps=16, learning_rate=8e-5, warmup_ratio=0.05,
                     logging_steps=50, eval_steps=800, save_steps=800,  weight_decay=0.0),
        "long": dict(num_train_epochs=3, per_device_train_batch_size=1, per_device_eval_batch_size=1,
                     gradient_accumulation_steps=32, learning_rate=5e-5, warmup_ratio=0.06,
                     logging_steps=50, eval_steps=1000, save_steps=1000, weight_decay=0.0),
    }
    p = profiles.get(profile, profiles["base"])

    # Build TrainingArguments with signature-check (4.55.2 safe)
    ta = dict(
        output_dir=out_dir,
        remove_unused_columns=False,
        bf16=True, fp16=False,
        gradient_checkpointing=True,
        optim="paged_adamw_8bit",
        lr_scheduler_type="cosine",
        save_total_limit=2,
        logging_dir=os.path.join(out_dir, "logs"),
        report_to="none",
        **p,
    )
    sig = inspect.signature(TrainingArguments.__init__)
    def maybe_set(k, v):
        if k in sig.parameters and v is not None:
            ta[k] = v

    eval_strategy = "steps" if eval_ds is not None else "no"
    maybe_set("evaluation_strategy", eval_strategy)  # older API
    maybe_set("eval_strategy", eval_strategy)        # newer API (some 4.5x branches)
    maybe_set("load_best_model_at_end", bool(eval_ds))
    maybe_set("metric_for_best_model", "eval_loss" if eval_ds is not None else None)
    maybe_set("greater_is_better", False if eval_ds is not None else None)

    train_args = TrainingArguments(**ta)

    init_kwargs = dict(
        model=model, args=train_args,
        train_dataset=train_ds, eval_dataset=eval_ds,
        data_collator=collator,
    )
    # tokenizer/processing_class compatibility
    if "processing_class" in inspect.signature(Trainer.__init__).parameters:
        init_kwargs["processing_class"] = processor
    else:
        init_kwargs["tokenizer"] = tokenizer

    trainer = Trainer(**init_kwargs)
    trainer.train()

    # Save adapter + processor/tokenizer
    adapter_dir = os.path.join(out_dir, "adapter")
    os.makedirs(adapter_dir, exist_ok=True)
    model.save_pretrained(adapter_dir)
    processor.save_pretrained(out_dir)
    tokenizer.save_pretrained(out_dir)

    print(f"[Saved] adapter => {adapter_dir}")
    print(f"[Saved] processor/tokenizer => {out_dir}")
    return adapter_dir

# ========= CLI (optional) =========
def build_parser():
    p = argparse.ArgumentParser(description="Qwen2.5-VL-7B unified multitask fine-tuning (QLoRA)")
    p.add_argument("--base_model", type=str, default="Qwen/Qwen2.5-VL-7B-Instruct")
    p.add_argument("--train_path", type=str, required=True)
    p.add_argument("--valid_path", type=str, default=None)
    p.add_argument("--out_dir",   type=str, required=True)
    p.add_argument("--profile",   choices=["dev","base","long"], default="base")
    p.add_argument("--lora_r", type=int, default=64)
    p.add_argument("--lora_alpha", type=int, default=128)
    p.add_argument("--lora_dropout", type=float, default=0.05)
    return p

def main():
    args = build_parser().parse_args()
    train(
        base_model=args.base_model,
        train_path=args.train_path,
        valid_path=args.valid_path,
        out_dir=args.out_dir,
        profile=args.profile,
        lora_r=args.lora_r, lora_alpha=args.lora_alpha, lora_dropout=args.lora_dropout,
    )

if __name__ == "__main__":
    main()
